Learning Objectives
Apply optimization methods such as (Stochastic) Gradient Descent, Momentum, RMSProp and Adam
Use random minibatches to accelerate convergence and improve optimization
Describe the benefits of learning rate decay and apply it to your optimization